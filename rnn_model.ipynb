{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEEXd6ESu8G0"
      },
      "source": [
        "start_token = \"<|startoftext|>\"\n",
        "end_token = \"<|endoftext|>\"\n",
        "line_break_token = \"<|line_break|>\"\n",
        "verse_break_token = \"<|verse_break|>\"\n",
        "\n",
        "def strip_tokens(input_text, clean_text):\n",
        "  with open(input_text, \"r\") as input_f, open(clean_text, \"w\") as clean_f:\n",
        "\n",
        "    for line in input_f.read().splitlines():\n",
        "      clean_line = line\n",
        "\n",
        "      #Check each line for tokens\n",
        "      if line_break_token in line:\n",
        "        clean_line = clean_line.replace(line_break_token, \"\")\n",
        "      if start_token in line:\n",
        "        clean_line = clean_line.replace(start_token, \"\")\n",
        "      if end_token in line:\n",
        "        clean_line = clean_line.replace(end_token, \"\")\n",
        "      if verse_break_token in line:\n",
        "        clean_line = clean_line.replace(verse_break_token, \"\")\n",
        "\n",
        "      clean_f.write(clean_line + \"\\n\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwHjrg4ewbXb",
        "outputId": "4628e4c3-f0b2-4e9e-c1c6-5eacac9f0ce4"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive', force_remount=True) \n",
        "  \n",
        "def get_data_by_song(train_file, test_file):\n",
        "  delimiter = \"<|endoftext|>\"\n",
        "\n",
        "  #Split training data into separate songs\n",
        "  with open('/content/drive/My Drive/'+train_file, 'r') as f:\n",
        "      full_songs = [song + delimiter for song in f.read().strip().split(delimiter)]\n",
        "\n",
        "  train_data = list(map(lambda x : x.split(), full_songs))\n",
        "\n",
        "  #Split testing data into separate songs\n",
        "  with open('/content/drive/My Drive/'+test_file, 'r') as f:\n",
        "      full_songs_test = [song + delimiter for song in f.read().strip().split(delimiter)]\n",
        "\n",
        "  test_data = list(map(lambda x : x.split(), full_songs_test))\n",
        "\n",
        "  #Build vocabulary dictionary\n",
        "  vocab = dict()\n",
        "  counter = 0\n",
        "  train_ind = []\n",
        "  for song in train_data:\n",
        "      song_ind = []\n",
        "      for word in song:\n",
        "          if word not in vocab:\n",
        "              vocab[word] = counter\n",
        "              counter += 1\n",
        "          song_ind.append(vocab[word])\n",
        "      train_ind.append(song_ind)\n",
        "  \n",
        "  test_ind = []\n",
        "  for song in test_data:\n",
        "      song_ind = []\n",
        "      for word in song:\n",
        "        if word not in vocab:\n",
        "          vocab[word] = counter\n",
        "          counter += 1\n",
        "        song_ind.append(vocab[word])\n",
        "      test_ind.append(song_ind)\n",
        "  \n",
        "  return train_ind, test_ind, vocab"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4IuD7F8wrZL",
        "outputId": "6ab5ad82-fa97-4e93-d93e-f008c99a4604"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "# from preprocess import get_data\n",
        "# from preprocess import get_data_by_song\n",
        "import copy\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"\n",
        "        The Model class predicts the next words in a sequence.\n",
        "\n",
        "        :param vocab_size: The number of unique words in the data\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "        # initialize vocab_size, embedding_size\n",
        "        self.vocab_size = vocab_size  \n",
        "        self.embedding_size = 64\n",
        "        self.batch_size = 64\n",
        "\n",
        "        # initialize embeddings and forward pass weights (weights, biases)\n",
        "        self.E = tf.Variable(tf.random.normal(\n",
        "            [self.vocab_size, self.embedding_size], stddev=0.1))\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            100, return_sequences=True, return_state=True)\n",
        "        self.dense1 = tf.keras.layers.Dense(512, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(\n",
        "            self.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, initial_state):\n",
        "        \"\"\"\n",
        "        :param inputs: word ids of shape (batch_size, window_size)\n",
        "        :param initial_state: 2-d array of shape (batch_size, rnn_size) as a tensor\n",
        "        :return: the batch element probabilities as a tensor, a final_state \n",
        "        using LSTM and only the probabilites as a tensor and a final_state as a tensor when using GRU\n",
        "        \"\"\"\n",
        "        embeddings = tf.nn.embedding_lookup(self.E, inputs)\n",
        "\n",
        "        output, mem_output, carry_output = self.lstm(\n",
        "            embeddings, initial_state=initial_state)\n",
        "        layer1out = self.dense1(output)\n",
        "        layer2out = self.dense2(layer1out)\n",
        "\n",
        "        return layer2out, (mem_output, carry_output)\n",
        "\n",
        "    def loss(self, probs, labels):\n",
        "        \"\"\"\n",
        "        Calculates average cross entropy sequence to sequence loss of the prediction\n",
        "\n",
        "        :param logits: a matrix of shape (batch_size, window_size, vocab_size) as a tensor\n",
        "        :param labels: matrix of shape (batch_size, window_size) containing the labels\n",
        "        :return: the loss of the model as a tensor of size 1\n",
        "        \"\"\"\n",
        "\n",
        "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n",
        "\n",
        "\n",
        "def train(model, train_inputs, train_labels):\n",
        "    \"\"\"\n",
        "    Runs through one epoch - all training examples.\n",
        "\n",
        "    :param model: the initilized model to use for forward and backward pass\n",
        "    :param train_inputs: train inputs (all inputs for training) of shape (num_inputs,)\n",
        "    :param train_labels: train labels (all labels for training) of shape (num_labels,)\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    prev_state = None\n",
        "\n",
        "    #Batching by song\n",
        "    for song_input, song_label in zip(train_inputs, train_labels):\n",
        "        batch_inputs = song_input[:-1]\n",
        "        batch_labels = song_label[1:]\n",
        "        batch_inputs = tf.reshape(batch_inputs, (-1, len(song_input)-1))\n",
        "        batch_inputs = tf.dtypes.cast(batch_inputs, tf.int32)\n",
        "        batch_labels = tf.reshape(batch_labels, (-1, len(song_label)-1))\n",
        "        batch_labels = tf.dtypes.cast(batch_labels, tf.int32)\n",
        "        if batch_inputs.shape[1] < 1:\n",
        "            continue\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            probs, prev_state = model.call(batch_inputs, prev_state)\n",
        "            loss = model.loss(probs, batch_labels)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        model.optimizer.apply_gradients(\n",
        "            zip(gradients, model.trainable_variables))\n",
        "\n",
        "\n",
        "def test(model, test_inputs, test_labels):\n",
        "    \"\"\"\n",
        "    Runs through one epoch - all testing examples\n",
        "\n",
        "    :param model: the trained model to use for prediction\n",
        "    :param test_inputs: train inputs (all inputs for testing) of shape (num_inputs,)\n",
        "    :param test_labels: train labels (all labels for testing) of shape (num_labels,)\n",
        "    :returns: perplexity of the test set\n",
        "    \"\"\"\n",
        "\n",
        "    prev_state = None\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for song_input, song_label in zip(test_inputs, test_labels):\n",
        "        batch_inputs = song_input[:-1]\n",
        "        batch_labels = song_label[1:]\n",
        "        batch_inputs = tf.reshape(batch_inputs, (-1, len(song_input)-1))\n",
        "        batch_inputs = tf.dtypes.cast(batch_inputs, tf.int32)\n",
        "        batch_labels = tf.reshape(batch_labels, (-1, len(song_label)-1))\n",
        "        batch_labels = tf.dtypes.cast(batch_labels, tf.int32)\n",
        "        if batch_inputs.shape[1] < 1:\n",
        "            continue\n",
        "\n",
        "        probs, prev_state = model.call(batch_inputs, prev_state)\n",
        "        loss = model.loss(probs, batch_labels)\n",
        "\n",
        "        total_loss += loss\n",
        "        num_batches += 1\n",
        "\n",
        "    return tf.math.exp(total_loss / num_batches)\n",
        "\n",
        "\n",
        "def generate_sentence(word1, length, vocab, model, sample_n=10):\n",
        "    \"\"\"\n",
        "    Takes a model, vocab, selects from the most likely next word from the model's distribution\n",
        "\n",
        "    :param model: trained RNN model\n",
        "    :param vocab: dictionary, word to id mapping\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "    previous_state = None\n",
        "\n",
        "    first_string = word1\n",
        "    first_word_index = vocab[word1]\n",
        "    next_input = [[first_word_index]]\n",
        "    text = [first_string]\n",
        "\n",
        "    for i in range(length):\n",
        "        logits, previous_state = model.call(next_input, previous_state)\n",
        "        logits = np.array(logits[0, -1, :])\n",
        "        top_n = np.argsort(logits)[-sample_n:]\n",
        "        n_logits = np.exp(logits[top_n])/np.exp(logits[top_n]).sum()\n",
        "        out_index = np.random.choice(top_n, p=n_logits)\n",
        "\n",
        "        text.append(reverse_vocab[out_index])\n",
        "        next_input[0].append(out_index)\n",
        "\n",
        "    final = \" \".join(text)\n",
        "\n",
        "    #Add newlines at different tokens\n",
        "    verses_with_breaks = list(map(lambda x: x.replace(\"<|line_break|>\", \"<|line_break|>\\n\"),\n",
        "                                  final.split(\"<|verse_break|>\")))\n",
        "    final = \"\\n<|verse_break|>\\n\".join(verses_with_breaks)\n",
        "    final = final.replace(\"<|startoftext|>\", \"<|startoftext|>\\n\")\n",
        "    final = final.replace(\"<|endoftext|>\", \"\\n<|endoftext|>\\n\")\n",
        "\n",
        "    print(\"sentence generated\")\n",
        "    return final\n",
        "  \n",
        "def get_results(result_file, vocab, model):\n",
        "    results = open(result_file, mode='w')\n",
        "\n",
        "    #Write generated lyrics to results file\n",
        "    for i in range(40):\n",
        "      results.write(generate_sentence(\"<|startoftext|>\", 1024, vocab, model) + \"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Pre-process and vectorize the data\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    train_data, test_data, vocab = get_data_by_song(\"data_repeat_tokens.txt\", \"test.txt\")\n",
        "\n",
        "    train_inputs = copy.copy(train_data)\n",
        "    train_inputs = np.array(train_inputs)\n",
        "    train_labels = copy.copy(train_data)\n",
        "    train_labels = np.array(train_labels)\n",
        "\n",
        "    test_inputs = copy.copy(test_data)\n",
        "    test_inputs = np.array(test_inputs)\n",
        "    test_labels = copy.copy(test_data)\n",
        "    test_labels = np.array(test_labels)\n",
        "\n",
        "    # Initialize model\n",
        "    model = Model(len(vocab))\n",
        "\n",
        "    # Training\n",
        "    print(\"Training...\")\n",
        "    for i in range(5):\n",
        "        print(\"Epoch: \", i)\n",
        "        train(model, train_inputs, train_labels)\n",
        "\n",
        "    # Testing\n",
        "    print(\"Testing...\")\n",
        "    perp = test(model, test_inputs, test_labels)\n",
        "\n",
        "    get_results(\"results.txt\", vocab, model)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training...\n",
            "Epoch:  0\n",
            "Epoch:  1\n",
            "Epoch:  2\n",
            "Epoch:  3\n",
            "Epoch:  4\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n",
            "sentence generated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KtDPtaHDesW"
      },
      "source": [
        "strip_tokens(\"/content/drive/My Drive/data_repeat_tokens.txt\", \"clean_data.txt\")\n",
        "strip_tokens(\"results.txt\", \"clean_results.txt\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a96wFo2MDyRp",
        "outputId": "5a7b00f0-1080-41e8-d7c8-0cebbc2438f7"
      },
      "source": [
        "!pip install text-matcher\n",
        "!pip install nltk\n",
        "!python -m nltk.downloader stopwords"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: text-matcher in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from text-matcher) (1.1.0)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.6/dist-packages (from text-matcher) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from text-matcher) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->text-matcher) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "903aiQW3DnmS",
        "outputId": "11f3dfa5-1522-4b9a-bf55-69d7ba0d4a01"
      },
      "source": [
        "!rm -rf log.csv\n",
        "!text-matcher clean_data.txt clean_results.txt -l log.csv"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 total matches found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e54-x1_zDsv8",
        "outputId": "fcab0796-3297-4c21-fe03-595ba76d6187"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def get_location_tuple(location_str):\n",
        "  location_lst = location_str.split(\"] [\")\n",
        "\n",
        "  replace_markers = location_lst[0].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\"), \", \"|\").replace(\")\", \"\")\n",
        "  convert_int = [(int(s.split(\", \")[0]), int(s.split(\", \")[1])) for s in replace_markers.split(\"|\")]\n",
        "\n",
        "  return convert_int\n",
        "\n",
        "def calc_char_diff(location_tuple):\n",
        "  return location_tuple[1] - location_tuple[0]\n",
        "\n",
        "with open('log.csv', newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      match_lst_A = get_location_tuple(row['Locations in A'])\n",
        "      match_lst_B = get_location_tuple(row['Locations in B'])\n",
        "\n",
        "      num_matches = int(row['Num Matches'])\n",
        "      generated_text_len = float(row['Text B Length'])\n",
        "\n",
        "    total_plagiarized = 0\n",
        "    for match_idx in range(num_matches):\n",
        "      print(\"\\nMatch \", match_idx + 1)\n",
        "\n",
        "      total_plagiarized += calc_char_diff(match_lst_B[match_idx])\n",
        "      print(\"Number of characters plagiarized: \", total_plagiarized)\n",
        "\n",
        "    percent_plagiarized = total_plagiarized / float(generated_text_len)\n",
        "    print(\"\\nPercentage plagiarized from the corpus: \", percent_plagiarized)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Match  1\n",
            "Number of characters plagiarized:  175\n",
            "\n",
            "Percentage plagiarized from the corpus:  0.017438963627304436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR_6UZ7kD3go",
        "outputId": "d918dab7-a464-4d68-9b3e-43d60f26542c"
      },
      "source": [
        "import math\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "WORD = re.compile(r\"\\w+\")\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
        "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    words = WORD.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "with open(\"/content/drive/My Drive/data_repeat_tokens.txt\", \"r\") as big_corpus, open(\"results.txt\", \"r\") as results:\n",
        "  vector1 = text_to_vector(big_corpus.read())\n",
        "  vector2 = text_to_vector(results.read())\n",
        "\n",
        "  cosine = get_cosine(vector1, vector2)\n",
        "\n",
        "print(\"Cosine similarity:\", cosine)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity: 0.8644577560518606\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}