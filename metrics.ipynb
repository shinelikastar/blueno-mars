{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metrics",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSxJFvYsthj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca67afe-a165-4bcb-dcad-2c57c5d05e1d"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "!pip install text-matcher\n",
        "!pip install nltk\n",
        "!python -m nltk.downloader stopwords\n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Requirement already satisfied: text-matcher in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.6/dist-packages (from text-matcher) (7.1.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from text-matcher) (1.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from text-matcher) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->text-matcher) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsPmdM8aKoN"
      },
      "source": [
        "In order for metrics to run, upload one file containing the corpus of original songs in a file called `with_repeat_data.txt`, and the results generated from your model of choice in `results.txt`. Both must be contained at the root directory of your Google Drive. Note that `with_repeat_data.txt` is the corpus with repeat tokens added in. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx1bgXaAtBCb",
        "outputId": "aed9ce78-57ca-46f9-e75e-a6b18d7e1040"
      },
      "source": [
        "gpt2.mount_gdrive()\n",
        "gpt2.copy_file_from_gdrive(\"with_repeat_data.txt\")\n",
        "gpt2.copy_file_from_gdrive(\"results.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FaCtjaGagRc"
      },
      "source": [
        "The code below strips the tokens (but not UNKs) from a given text so the tokens do not count towards plagiarized character count. It's run on both documents before comparing similarity. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JxwJ3uhu3KD"
      },
      "source": [
        "start_token = \"<|startoftext|>\"\n",
        "end_token = \"<|endoftext|>\"\n",
        "line_break_token = \"<|line_break|>\"\n",
        "verse_break_token = \"<|verse_break|>\"\n",
        "\n",
        "def strip_tokens(input_text, clean_text):\n",
        "  with open(input_text, \"r\") as input_f, open(clean_text, \"w\") as clean_f:\n",
        "    for line in input_f.read().splitlines():\n",
        "      clean_line = line\n",
        "      if line_break_token in line:\n",
        "        clean_line = clean_line.replace(line_break_token, \"\")\n",
        "      if start_token in line:\n",
        "        clean_line = clean_line.replace(start_token, \"\")\n",
        "      if end_token in line:\n",
        "        clean_line = clean_line.replace(end_token, \"\")\n",
        "      if verse_break_token in line:\n",
        "        clean_line = clean_line.replace(verse_break_token, \"\")\n",
        "      \n",
        "      clean_f.write(clean_line + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anGMgRvlhnwv"
      },
      "source": [
        "strip_tokens(\"data.txt\", \"clean_data.txt\")\n",
        "strip_tokens(\"results.txt\", \"clean_results.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtX8ciaqatD5"
      },
      "source": [
        "The script below logs the matches into a `csv` file that is parsed to retrieve the matches. Note that `text-matcher` appends to the log file passed in, so the first line removes it before running on the two documents. \n",
        "\n",
        "Matches will be highlighted in red. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hh-pEtJSzSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3370df11-b009-4081-e179-e88987052d10"
      },
      "source": [
        "!rm -rf log.csv\n",
        "!text-matcher clean_data.txt clean_results.txt -l log.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 total matches found.\n",
            "Extending match forwards with words: oh oh\n",
            "Extending match forwards with words: unk unk\n",
            "Extending match forwards with words: oh oh\n",
            "Extending match forwards with words: oh oh\n",
            "Extending match forwards with words: oh oh\n",
            "\n",
            "\n",
            "match 1:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (1083756, 1083798) aye singing oh oh-oh \u001b[31moh-oh-oh oh-oh <|UNK|> oh-oh-oh-oh-oh-oh\u001b[0m oh ooh yeah if we could throw\n",
            "\u001b[32mclean_results.txt\u001b[0m: (4177, 4240) life of a <|UNK|> oh yeah <|repeat \u001b[31moh oh oh oh oh my my my <|UNK|> oh oh my <|UNK|> oh oh my oh\u001b[0m oh <|UNK|> just a day a day a day\n",
            "\n",
            "\n",
            "match 2:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (1518589, 1518651) fire oh oh oh i'm on fire \u001b[31mooh-ooh ooh ooh-ooh ooh-ooh ooh-ooh-ooh ooh-ooh ooh ooh ooh\u001b[0m ooh-ooh ooh-ooh-ooh\n",
            "\u001b[32mclean_results.txt\u001b[0m: (16169, 16231) get in my bed because i am the one and you are the other \u001b[31mooh ooh ooh ooh ooh ooh ooh ooh ooh ooh ooh ooh ooh ooh ooh\u001b[0m way you keep me coming keep me coming\n",
            "\n",
            "\n",
            "match 3:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (5377088, 5377160) way you can call it another lonely day \u001b[31mgo your own way go your own way you can go your own way go your own way\u001b[0m call it another lonely day you can go\n",
            "\u001b[32mclean_results.txt\u001b[0m: (25578, 25668) see you just love and i see you make it your \u001b[31mgo your own way you can go your own way you can go your own way you can go your own way\u001b[0m go your own way oh it seems\n",
            "\n",
            "\n",
            "match 4:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (5580083, 5580140) way you can call it another lonely day \u001b[31mgo your own way go your own way you can go your own way\u001b[0m go your own way you can call it another\n",
            "\u001b[32mclean_results.txt\u001b[0m: (26559, 26624) see you just love and i see you make it your \u001b[31mgo your own way you can go your own way you can go your own way\u001b[0m UNK|> <| <|UNK|> i'll be your first <|UNK\n",
            "\n",
            "\n",
            "match 5:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (14831175, 14831301) make it all your own so when we are apart \u001b[31mnever be alone you'll never be alone you'll never be alone you'll never be alone you'll never be alone you'll never be alone\u001b[0m take a piece of my heart and make it all\n",
            "\u001b[32mclean_results.txt\u001b[0m: (29618, 29819) can't believe you i can't believe what you said \u001b[31mnever alone when you're here you're never alone when you're here you're never alone when you're here you're never alone when you're here you're never alone when you're here you're never alone\u001b[0m never alone and when you're here when you're here you're always alone when you're here you're never\n",
            "\n",
            "\n",
            "match 6:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (17104951, 17105018) maybe i grew up a little too soon <|UNK \u001b[31mrepeat|> <|repeat|> <|repeat|> <|repeat|> <|UNK|> <|UNK|> the edge\u001b[0m UNK|> the edge <|UNK|> i almost <|UNK\n",
            "\u001b[32mclean_results.txt\u001b[0m: (38475, 38538) UNK|> <|UNK|> <|UNK|> <|repeat|> <|repeat \u001b[31mrepeat|> <|repeat|> <|repeat|> <|repeat|> <|UNK|> <|UNK|> <|UNK\u001b[0m UNK|> <|UNK|> <|UNK|> <|UNK|> <|UNK\n",
            "\n",
            "\n",
            "match 7:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (17385503, 17385574) rise up oh rise up you gotta rise \u001b[31mlook like look like what it do what it do what it look like look like\u001b[0m mc holla at the people one time got\n",
            "\u001b[32mclean_results.txt\u001b[0m: (40114, 40293) people are born kings it's like \u001b[31mlooking at me and i just it's like that you're looking at me and i just it's like that you're looking at me and i just it's like that you're looking at me and i just it's like\u001b[0m looking at me and i just it's like that you are looking at me and i just it's like\n",
            "\n",
            "\n",
            "match 8:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (17580639, 17580793) gonna make it you're gonna make it you're gonna \u001b[31mmake it you you can make it you can make it you you you can make it you can make it you you you can make it you can make it make it make make make\u001b[0m believe i once was lost but now i'm found i got\n",
            "\u001b[32mclean_results.txt\u001b[0m: (53675, 53760) make it or you can make it wrong but it's the best that you can do <|repeat \u001b[31mmake it make it make it make it make it make it make it make it make it make it make\u001b[0m say whatever i want <| don't tell me that you love\n",
            "\n",
            "\n",
            "match 9:\n",
            "\u001b[32mclean_data.txt\u001b[0m: (19293294, 19293323) ooh-ooh ooh ooh-ooh \u001b[31mooh ooh-ooh ooh-ooh-ooh got\u001b[0m gypsy blood always chasing thunder\n",
            "\u001b[32mclean_results.txt\u001b[0m: (70417, 70451) things i've got the things i've got the things \u001b[31mooh ooh ooh ooh ooh ooh i've got\u001b[0m things i've got the things i've got the things\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wu_uOZEbEnp"
      },
      "source": [
        "The following code looks complicated but is just parsing the csv generated by `text-matcher` to retrieve the locations of where the characters matched, calculate their difference, and find the percentage in the generated song document that was plagiarized from the original document. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08CTO2rsdtg-",
        "outputId": "d1f58fa4-96a1-411e-a5ff-4d45c404ba15"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def get_location_tuple(location_str):\n",
        "  location_lst = location_str.split(\"] [\")\n",
        "\n",
        "  replace_markers = location_lst[0].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\"), \", \"|\").replace(\")\", \"\")\n",
        "  convert_int = [(int(s.split(\", \")[0]), int(s.split(\", \")[1])) for s in replace_markers.split(\"|\")]\n",
        "\n",
        "  return convert_int\n",
        "\n",
        "def calc_char_diff(location_tuple):\n",
        "  return location_tuple[1] - location_tuple[0]\n",
        "\n",
        "with open('log.csv', newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      match_lst_A = get_location_tuple(row['Locations in A'])\n",
        "      match_lst_B = get_location_tuple(row['Locations in B'])\n",
        "\n",
        "      num_matches = int(row['Num Matches'])\n",
        "      generated_text_len = float(row['Text B Length'])\n",
        "\n",
        "    total_plagiarized = 0\n",
        "    for match_idx in range(num_matches):\n",
        "      print(\"\\nMatch \", match_idx + 1)\n",
        "\n",
        "      total_plagiarized += calc_char_diff(match_lst_B[match_idx])\n",
        "      print(\"Number of characters plagiarized: \", total_plagiarized)\n",
        "\n",
        "    percent_plagiarized = total_plagiarized / float(generated_text_len)\n",
        "    print(\"\\nPercentage plagiarized from the corpus: \", percent_plagiarized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Match  1\n",
            "Number of characters plagiarized:  63\n",
            "\n",
            "Match  2\n",
            "Number of characters plagiarized:  125\n",
            "\n",
            "Match  3\n",
            "Number of characters plagiarized:  215\n",
            "\n",
            "Match  4\n",
            "Number of characters plagiarized:  280\n",
            "\n",
            "Match  5\n",
            "Number of characters plagiarized:  481\n",
            "\n",
            "Match  6\n",
            "Number of characters plagiarized:  544\n",
            "\n",
            "Match  7\n",
            "Number of characters plagiarized:  723\n",
            "\n",
            "Match  8\n",
            "Number of characters plagiarized:  808\n",
            "\n",
            "Match  9\n",
            "Number of characters plagiarized:  842\n",
            "\n",
            "Percentage plagiarized from the corpus:  0.01137637982516585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cakkleeCZKNL"
      },
      "source": [
        "The below code is sourced from [this Stack Overflow post](https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings). It measures the surface similarity of two texts. We  vectorize our generated songs and original corpus as a vocabulary distribution in order to represent how “pop-like” our generated lines are. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieSqt0P9Owpy",
        "outputId": "8b45771b-8462-417b-c06b-d7fd5c3565fc"
      },
      "source": [
        "import math\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "WORD = re.compile(r\"\\w+\")\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
        "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    words = WORD.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "with open(\"data.txt\", \"r\") as big_corpus, open(\"results.txt\", \"r\") as results:\n",
        "  vector1 = text_to_vector(big_corpus.read())\n",
        "  vector2 = text_to_vector(results.read())\n",
        "\n",
        "  cosine = get_cosine(vector1, vector2)\n",
        "\n",
        "print(\"Cosine similarity:\", cosine)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity: 0.9556724155306071\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}